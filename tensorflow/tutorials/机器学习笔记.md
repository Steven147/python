# 机器学习

> [吴恩达机器学习-网易云课堂](https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1049052745&courseId=1004570029)
>
> 0212-李可-机器学习概述
>
> [【中英字幕】吴恩达深度学习课程第一课 — 神经网络与深度学习](https://www.bilibili.com/video/av66314465?p=36)
>
> [Tensorflow](https://tensorflow.google.cn/tutorials/)

## 1

### 1.1 欢迎

### 1.2 机器学习是什么

### 1.3 `监督学习`

其基本思想是，我们数据集中的`每个样本都有相应的“正确答案”`，再根据这些样本作出预测。

我们还介绍了回归问题，即通过回归来推出一个`连续`的输出，

- 预测房价：横轴表示房子的面积，单位是平方英尺，纵轴表示房价，单位是千美元
- 你有一大批同样的货物，想象一下，你有上千件一模一样的货物等待出售，这时你想预测接下来的三个月能卖多少件？

之后我们介绍了分类问题，其目标是推出一组`离散`的结果。

- 横轴表示肿瘤的大小，纵轴上，我标出1和0表示是或者不是恶性肿瘤
- 你有许多客户，这时你想写一个软件来检验每一个用户的账户。对于每一个账户，你要判断它们是否曾经被盗过？
- 区别垃圾还是非垃圾邮件

### 1.4 无监督学习

即无监督学习中没有任何的标签
交给算法大量的数据，并让算法为我们从数据中找出某种结构。

- 聚类算法,我不知道数据里面有什么。因为我们没有给算法正确答案来回应数据集中的数据，所以这就是无监督学习。
- 鸡尾酒宴问题

## 2 单变量线性回归(Linear Regression with One Variable)

### 2.1 模型表示

### 2.2 代价函数

### 2.3 代价函数的直观理解I

### 2.4 代价函数的直观理解II

### 2.5 梯度下降

### 2.6 梯度下降的直观理解

### 2.7 梯度下降的线性回归

### 2.8 接下来的内容

## 3 线性代数回顾(Linear Algebra Review)

### 3.1 矩阵和向量

### 3.2 加法和标量乘法

### 3.3 矩阵向量乘法

### 3.4 矩阵乘法

### 3.5 矩阵乘法的性质

### 3.6 逆、转置

## 4 多变量线性回归(Linear Regression with Multiple Variables)

### 4.1 多维特征

### 4.2 多变量梯度下降

### 4.3 梯度下降法实践1-特征缩放

### 4.4 梯度下降法实践2-学习率

### 4.5 特征和多项式回归

### 4.6 正规方程

### 4.7 正规方程及不可逆性（选修）

## 5 Octave教程(Octave Tutorial)

### 5.1 基本操作

### 5.2 移动数据

### 5.3 计算数据

### 5.4 绘图数据

### 5.5 控制语句：for，while，if语句

### 5.6 向量化 88

### 5.7 工作和提交的编程练习

## 6 逻辑回归(Logistic Regression)

逻辑回归算法是`分类`算法

### 6.1 分类问题

### 6.2 假说表示：$h_\theta \left( x \right)$意义

- 该模型的输出变量范围始终在0和1之间。
- 逻辑回归模型的假设是：
- $h_\theta \left( x \right)=g\left(\theta^{T}X \right)$
- $h_\theta \left( x \right)$的作用是，对于给定的输入变量，根据选择的参数计算输出变量=1的可能性（**estimated probablity**）
- 即$h_\theta \left( x \right)=P\left( y=1|x;\theta \right)$
- 例如，如果对于给定的$x$，通过已经确定的参数计算得出$h_\theta \left( x \right)=0.7$，则表示有70%的几率$y$为正向类

### 6.3 判定边界 decision boundary

- 现在假设我们有一个模型：
- $h_\theta \left( x \right)=g\left(\theta^{T}X \right)$
- $g\left( z \right)=\frac{1}{1+{{e}^{-z}}}$
- 并且已经得出参数参数$\theta$ 是向量[-3 1 1]。 则当$-3+{x_1}+{x_2} \geq 0$，即${x_1}+{x_2} \geq 3$时，模型将预测 $y=1$
- 直线 ${x_1}+{x_2} = 3$，这条线便是我们模型的分界线†
- 要获得圆形区域 ${h_\theta}\left( x \right)=g\left( {\theta_0}+{\theta_1}{x_1}+{\theta_{2}}{x_{2}}+{\theta_{3}}x_{1}^{2}+{\theta_{4}}x_{2}^{2} \right)$ 参数是[-1 0 0 1 1]
- 决策边界是假设函数的一个性质，由向量参数和变量类型决定

### 6.4 代价函数

- $J\left( \theta  \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{{Cost}\left( {h_\theta}\left( {x}^{\left( i \right)} \right),{y}^{\left( i \right)} \right)}$
- 为得到凸函数$J\left( \theta  \right)$
- $J\left( \theta  \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\log \left( {h_\theta}\left( {{x}^{(i)}} \right) \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1-{h_\theta}\left( {{x}^{(i)}} \right) \right)]}$

- 凸函数容易找到全局最小值（使用梯度下降法

   ```python
   import numpy as np
   def cost(theta, X, y):
      theta = np.matrix(theta)
      X = np.matrix(X)
      y = np.matrix(y)
      first = np.multiply(-y, np.log(sigmoid(X* theta.T)))
      second = np.multiply((1 - y), np.log(1 - sigmoid(X* theta.T)))
      return np.sum(first - second) / (len(X))
   ```

### 6.5 简化的成本函数和梯度下降

- $\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j} J(\theta)$
- $\theta_j := \theta_j - \alpha \frac{1}{m}\sum\limits_{i=1}^{m}{{\left( {h_\theta}\left( \mathop{x}^{\left( i \right)} \right)-\mathop{y}^{\left( i \right)} \right)}}\mathop{x}_{j}^{(i)}$

### 6.6 高级优化

### 6.7 多类别分类：一对多

## 7 正则化(Regularization)

### 7.1 过拟合的问题

### 7.2 代价函数

### 7.3 正则化线性回归

### 7.4 正则化的逻辑回归模型

## 8 神经网络：表述(Neural Networks: Representation)

### 8.1 非线性假设

### 8.2 神经元和大脑

### 8.3 模型表示1

### 8.4 模型表示2

### 8.5 样本和直观理解1

### 8.6 样本和直观理解II

### 8.7 多类分类

## 9 神经网络的学习(Neural Networks: Learning)

### 9.1 代价函数

### 9.2 反向传播算法

### 9.3 反向传播算法的直观理解

### 9.4 实现注意：展开参数

### 9.5 梯度检验

### 9.6 随机初始化

### 9.7 综合起来

### 9.8 自主驾驶

## 10 应用机器学习的建议(Advice for Applying Machine Learning)

### 10.1 决定下一步做什么

### 10.2 评估一个假设

### 10.3 模型选择和交叉验证集

### 10.4 诊断偏差和方差

### 10.5 正则化和偏差/方差

### 10.6 学习曲线

### 10.7 决定下一步做什么

## 11 机器学习系统的设计(Machine Learning System Design)

### 11.1 首先要做什么

### 11.2 误差分析

### 11.3 类偏斜的误差度量

### 11.4 查准率和查全率之间的权衡

### 11.5 机器学习的数据

## 12 支持向量机(Support Vector Machines)

### 12.1 优化目标

从逻辑回归开始展示我们如何一点一点修改来得到本质上的`支持向量机`。`监督学习算法`。`线性分类模型`

### 12.2 大边界的直观理解

### 12.3 数学背后的大边界分类（选修）

### 12.4 核函数1

### 12.5 核函数2

### 12.6 使用支持向量机

## 13 聚类(Clustering)

### 13.1 无监督学习：简介

### 13.2 K-均值算法

### 13.3 优化目标

### 13.4 随机初始化

### 13.5 选择聚类数

## 14 降维(Dimensionality Reduction)

### 14.1 动机一：数据压缩

### 14.2 动机二：数据可视化

### 14.3 主成分分析问题

### 14.4 主成分分析算法

### 14.5 选择主成分的数量

### 14.6 重建的压缩表示

### 14.7 主成分分析法的应用建议

## 15 异常检测(Anomaly Detection)

### 15.1 问题的动机

### 15.2 高斯分布

### 15.3 算法

### 15.4 开发和评价一个异常检测系统

### 15.5 异常检测与监督学习对比

### 15.6 选择特征

### 15.7 多元高斯分布（选修）

### 15.8 使用多元高斯分布进行异常检测（选修）

## 16 推荐系统(Recommender Systems)

### 16.1 问题形式化

### 16.2 基于内容的推荐系统

### 16.3 协同过滤

### 16.4 协同过滤算法

### 16.5 向量化：低秩矩阵分解

### 16.6 推行工作上的细节：均值归一化

## 17 大规模机器学习(Large Scale Machine Learning)

### 17.1 大型数据集的学习

### 17.2 随机梯度下降法

### 17.3 小批量梯度下降

### 17.4 随机梯度下降收敛

### 17.5 在线学习

### 17.6 映射化简和数据并行

## 18 应用实例：图片文字识别(Application Example: Photo OCR)

### 18.1 问题描述和流程图

### 18.2 滑动窗口

### 18.3 获取大量数据和人工数据

### 18.4 上限分析：哪部分管道的接下去做

[【全-中文字幕】深度学习_吴恩达_DeepLearning.ai](https://www.bilibili.com/video/av49445369?p=30)

1. introduction
2. 单变量线性回归
3. 多变量线性回归
   1. binary classification 二分类
      - $$input: X \in R^{n_x\times m}$$
      - $$output:y \in \{0,1\},Y \in R^{1\times m}$$

   2. logistic regression 逻辑回归
      - $$x \in R^{n_x} ,  \omega \in R^{n_x}, b \in R $$
      - $$ z = \omega^Tx+b$$
      - $$  \hat{y}=a= S(z) \ \  where \ \ S_\theta (z) =  \frac{\mathrm{1} }{\mathrm{1} + e^{- \theta^Tz} }$$

   3. logistic regression cost function  逻辑回归损失函数
      - $$ Given\{(x^{(1)},y^{(1)}),\dots,(x^{(1)},y^{(1)})\},\ want\ \hat{y}^{(i)} \approx y^{(i)}$$
      - $$ Loss(error)\ function:\ \mathscr{L}(\hat{y},y)= - \ (\ y\ log\ \hat{y}+(1-y)\ log\ (1-\hat{y})\ )$$
      - $$ Cost\ function: J(\omega,b) = \frac{1}{m} \sum_{i=1}^{m}{\mathscr{L}(\hat{y}^{(i)},y^{(i)})}$$

   4. Gradient Descent 梯度下降法
      - $$want\ to\ find\ \omega,b\ that\ minimize\ J(\omega,b) $$
      - $$\omega = \omega - \alpha\frac{\partial J(\omega,b)}{\partial \omega},\ b = b - \alpha\frac{\partial J(\omega,b)}{\partial b}, \ \alpha:\ learning\ rate$$
   5. Computation Graph 计算图
   6. Derivatives with a Computation Graph 计算图的导数计算：**复合函数-链式法则**
   7. Logistic Regression Gradient descent 逻辑回归梯度下降法
      - $$ z = \omega^Tx+b$$
      - $$  \hat{y}=a= S(z)$$
      - $$\mathscr{L}(a,y)= - \ (\ y\ log\ a+(1-y)\ log\ (1-a)\ )$$
      - $$ "da" =  \frac{\partial \mathscr{L}(a,y)}{\partial a} = -\frac{y}{a}+\frac{1-y}{1-a} $$
      - $$ "dz" =  \frac{\partial \mathscr{L}(a,y)}{\partial a}\cdot \frac{\partial a}{\partial z} = (-\frac{y}{a}+\frac{1-y}{1-a})\cdot a(1-a)= a-y$$
      - $$ "d\omega" =  \frac{\partial \mathscr{L}}{\partial \omega}\ = x\cdot "dz"$$
      - $$ "db" =  \frac{\partial \mathscr{L}}{\partial b}\ = "db"$$
   8. Vectorization 向量化 [Vectorization demo.ipynb](/Vectorization%20demo.ipynb)

4. basics of neural network programing
5. one hidden layer neural networks
6. deep neural networks
